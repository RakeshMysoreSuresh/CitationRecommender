/**
 * 
 */
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.PrintWriter;
import java.io.StringReader;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.StopFilter;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.en.EnglishAnalyzer;
import org.apache.lucene.analysis.en.EnglishMinimalStemFilter;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.util.Version;

/**
 * @author Rakesh_BCKUP
 * Generates a bag of words from all the citations in a citing paper. Each line corresponds to one citing paper.
 * Inputs: These files can be generated by directly exporting a column in database table to a text file. TOAD is an
 * useful tool for such operations.
 * 1. ID.txt: Each entry corresponds to the paperID of the citing paper
 * 2. Title.txt: The title of the cited paper
 * 3. Context.txt: The citation context of that citation
 * 4. ProperNoun file(OPTIONAL): If the proper nouns are already separated from other words in the citation context,
 * 	                             it can be excluded from the aggressive preprocessing step and chosen to be added to
 * 	                             the bag of words later.
 * 5. Fertility(OPTIONAL): The constant MAX_FERTILITY can be modified as required
 * 
 * Outputs: Each line in this 
 * 1. Bag of words file
 */
public class BagOfWordsGenv2 {
	
	private static BufferedReader idReader;
	private static BufferedReader contextReader;
	private static BufferedReader properNounReader;
	private static PrintWriter wordsWriter;
	private Set<String> bagOfWords = new HashSet<String>();
	private Set<Integer> citedPaperIds = new HashSet<Integer>();
	private PrintWriter titleWriter;
	private BufferedReader titleReader;
	private HashMap<String, Integer> titlesMap;
	private int ignored = 0;
	static final int MAX_FERTILITY = 20;
	static final double LOWERBOUND = 1/MAX_FERTILITY; 

	public BagOfWordsGenv2(String[] args) throws IOException {
		
		if(args.length != 5){
			usage();
		}
		
		idReader = new BufferedReader(new FileReader(args[0]));
		titleReader = new BufferedReader(new FileReader(args[1]));
		contextReader = new BufferedReader(new FileReader(args[2]));
		
		// Buffered Writers
		wordsWriter = new PrintWriter(new FileWriter(args[3]));
		titleWriter = new PrintWriter(new FileWriter(args[4]));
	}
	
	public BagOfWordsGenv2(String[] args, String properNounFile) throws IOException {
		this(args);
		properNounReader = new BufferedReader(new FileReader(properNounFile));
	}
	/**
	 * @param args
	 * @throws IOException
	 * @throws ClassNotFoundException
	 */

	public static void main(String[] args) throws IOException,
			ClassNotFoundException {

		BagOfWordsGenv2 parallel = new BagOfWordsGenv2(new String[]
				{"ID.txt", "Title.txt", "Context.txt", "BagOfWordsPOSTag.txt", 
						"CitedPapersPOSTag.txt"});
		parallel.generate();

	}
	/**
	 * Gather all words from all citation contexts in a given citing paper and write into a line
	 * Gather all title IDs(numbers) of all cited paperds from a given citing paper and write into a line 
	 * @throws IOException
	 */
	public void generate() throws IOException {
		int numCitation = 0;
		String curPaperId, context, title, noun;
		String previousId = "";
		
		//Dummy reads to exclude the column name
		curPaperId = idReader.readLine(); // read the column name
		context = contextReader.readLine(); // read the column name
		title = titleReader.readLine(); // read the column name
		if(properNounReader!=null){
			noun = properNounReader.readLine();//Dummy read
		}
		System.out.println(curPaperId);
		long startTime = 0, endTime = 0;
		startTime = System.currentTimeMillis();
		int percentage = 0;
		while ((curPaperId = idReader.readLine()) != null) {
			numCitation++;

			if (0 == numCitation % 12000) {
				endTime = System.currentTimeMillis();
				System.out.println("Time taken for " + (++percentage) + "% = "
						+ ((endTime - startTime) / 1000));
				// break;
			}
			if (curPaperId.equals(previousId)) {

				// Read one citation context and get bag of words
				context = contextReader.readLine();
				removeStopWordsStem(context);

			} else {
				if (bagOfWords.size() != 0) {
					// write bagsOfWords to a file
					// write citedPaperIds to a file
					writeToFile(bagOfWords, wordsWriter);
				}
				
				// Read one citation context and get bag of words
				context = contextReader.readLine();
				// Initialize Sets to read the new Paper citations
				bagOfWords.clear();
				context = context.substring(context.indexOf(' '), context.lastIndexOf(' '));
				removeStopWordsStem(context);
				if(properNounReader!=null){
					noun = properNounReader.readLine();
					bagOfWords.addAll(Arrays.asList(noun.split(" ")));
				}	
			}

			previousId = curPaperId;

		}

		// Write the last set of values
		if (bagOfWords != null) {
			// write bagsOfWords to a file
			// write citedPaperIds to a file
			writeToFile(bagOfWords, wordsWriter);
		}

		wordsWriter.close();
	}
	
	private static void writeToFile(Set<String> bagOfWords, PrintWriter writer) {

		Iterator<String> it = bagOfWords.iterator();
		String write = "";
		while (it.hasNext()) {
			write = write + " " + it.next();
		}
		writer.println(write);
	}
	
	private void usage() {

		System.out.println("<ID file> <Title> <Raw Contexts> <Output BagofWords filename> <Output CitedPaperID>");
		System.exit(1);
	}

	private void writeToFile() {
		double fertility = bagOfWords.size()/citedPaperIds.size();
		
		if (!((fertility>MAX_FERTILITY)||(fertility<LOWERBOUND))) {
			Iterator<String> it = bagOfWords.iterator();
			String write = "";
			while (it.hasNext()) {
				write = write + " " + it.next();
			}
			wordsWriter.println(write);
			Iterator<Integer> it2 = citedPaperIds.iterator();
			write = "";
			while (it2.hasNext()) {
				write = write + " " + it2.next();
			}
			titleWriter.println(write);
		}
		else{
			ignored++;
		}
	}
	
	public void removeStopWordsStem(String input) throws IOException {
		// input string
		Version matchVersion = Version.LUCENE_35; // Substitute desired Lucene version 
		Analyzer analyzer = new StandardAnalyzer(matchVersion); // or any other analyzer
		TokenStream tokenStream = analyzer.tokenStream("test",
				new StringReader(input));
		// remove stop words
		tokenStream = new EnglishMinimalStemFilter(tokenStream);
		// retrieve the remaining tokens
		CharTermAttribute token = tokenStream
				.getAttribute(CharTermAttribute.class);
		if(token==null){
			int i=0;
		}
		while (tokenStream.incrementToken()) {
			bagOfWords.add(token.toString());
		}
		
		tokenStream.end();
		tokenStream.close();
		analyzer.close();
	}
	
/*	private void readTitleMap() throws ClassNotFoundException, IOException {

		FileInputStream fin = new FileInputStream("Map.ser");
		ObjectInputStream ois = new ObjectInputStream(fin);
		titlesMap = (HashMap<String, Integer>) ois.readObject();
	}*/
}
