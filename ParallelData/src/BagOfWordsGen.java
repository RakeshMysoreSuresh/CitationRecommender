/**
 *
 */
import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintWriter;
import java.io.StringReader;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import java.util.regex.Pattern;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.LowerCaseTokenizer;
import org.apache.lucene.analysis.PorterStemFilter;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.en.EnglishMinimalStemFilter;
import org.apache.lucene.analysis.en.KStemFilter;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.util.Version;

/**
 * @author Rakesh_BCKUP
 * Generates a bag of words from all the citations in a citing paper. Each line corresponds to one citing paper.
 * Inputs: These files can be generated by directly exporting a column in database table to a text file. TOAD is an
 * useful tool for such operations.
 * 1. ID.txt: Each entry corresponds to the paperID of the citing paper
 * 2. Title.txt: The title of the cited paper
 * 3. Context.txt: The citation context of that citation
 * 4. ProperNoun file(OPTIONAL): If the proper nouns are already separated from other words in the citation context,
 * 	                             it can be excluded from the aggressive preprocessing step and chosen to be added to
 * 	                             the bag of words later.
 * 5. Fertility(OPTIONAL): The constant MAX_FERTILITY can be modified as required
 *
 * Outputs: Each line in this
 * 1. Bag of words file
 */
public class BagOfWordsGen extends Config{

	//"<ID file> <Raw Contexts> <Output BagofWords filename> "
	public BagOfWordsGen(String[] args) {

/*		if(args.length != 5){
			usage();
		}*/
		try {
			idReader = new BufferedReader(new FileReader(args[0]));
			//titleReader = new BufferedReader(new FileReader(args[1]));
			contextReader = new BufferedReader(new FileReader(args[1]));
			wordsWriter = new PrintWriter(new FileWriter(args[2]));
			System.out.println("Creating bag of words at:" + args[2]);
			readNoisyWords();
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}

	public BagOfWordsGen(String[] args, String properNounFile){
		this(args);
		try {
			properNounReader = new BufferedReader(new FileReader(properNounFile));
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}
	
	/**
	 * @param args
	 * @throws IOException
	 * @throws ClassNotFoundException
	 */

	public static void main(String[] args) throws IOException {

		stemContext();
	}
	/**
	 * Gather all words from all citation contexts in a given citing paper and write into a line
	 * Gather all title IDs(numbers) of all cited paperds from a given citing paper and write into a line
	 * @throws IOException
	 */
	public void generate() {
		int numCitation = 0;
		String curPaperId, context, noun; //title;
		String previousId = "";

		try {
			//Dummy reads to exclude the column name
			//curPaperId = idReader.readLine(); // read the column name
			//context = contextReader.readLine(); // read the column name
			//title = titleReader.readLine(); // read the column name
			if(properNounReader!=null){
				noun = properNounReader.readLine();//Dummy read
			}
			//System.out.println(curPaperId);
			long startTime = 0, endTime = 0;
			startTime = System.currentTimeMillis();
			int percentage = 0;
			while ((curPaperId = idReader.readLine()) != null) {
				numCitation++;

				if (0 == numCitation % 12000) {
					endTime = System.currentTimeMillis();
					System.out.println("Time taken for " + (++percentage) + "% = "
							+ ((endTime - startTime) / 1000));
					// break;
				}
				if (curPaperId.equals(previousId) || numCitation == 1) {

					// Read one citation context and get bag of words
					context = contextReader.readLine();
					removeStopWordsStem(context);
					if(!Config.AGGRESSIVE_STEMMING)
					{
						removeStopWordsStem(context);
					}
					else
					{
						aggresivelyStem(context);
					}

				} else {
					if (bagOfWords.size() != 0) {
						// write bagsOfWords to a file
						// write citedPaperIds to a file
						writeToFile(bagOfWords, wordsWriter);
					}

					// Read one citation context and get bag of words
					context = contextReader.readLine();
					// Initialize Sets to read the new Paper citations
					bagOfWords.clear();

					if(Config.MINIMAL_STEMMING)
					{
						removeStopWordsStem(context);
					}
					else if(Config.AGGRESSIVE_STEMMING)
					{
						aggresivelyStem(context);
					}
					if (properNounReader != null) {
						noun = properNounReader.readLine();
						bagOfWords.addAll(Arrays.asList(noun.split(" ")));
					}
				}
				previousId = curPaperId;
			}

			// Write the last set of values
			if (bagOfWords != null) {
				// write bagsOfWords to a file
				// write citedPaperIds to a file
				writeToFile(bagOfWords, wordsWriter);
			}

			wordsWriter.close();
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}

	private static void writeToFile(Set<String> bagOfWords, PrintWriter writer) {

		Iterator<String> it = bagOfWords.iterator();
		String write = "";
		while (it.hasNext()) {
			write = write + " " + it.next();
		}
		writer.println(write);
	}

	private void removeStopWordsStem(String input) throws IOException {
		// input string
		Version matchVersion = Version.LUCENE_35; // Substitute desired Lucene version
		Analyzer analyzer = new StandardAnalyzer(matchVersion); // or any other analyzer
		TokenStream tokenStream = analyzer.tokenStream("test",new StringReader(input));
		// remove stop words
		tokenStream = new EnglishMinimalStemFilter(tokenStream);
		// retrieve the remaining tokens
		CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);

		while (tokenStream.incrementToken()) {
			bagOfWords.add(token.toString());
		}

		tokenStream.end();
		tokenStream.close();
		analyzer.close();
	}
	
	void aggresivelyStem (String input) throws IOException {
			
		Version matchVersion = Version.LUCENE_35; 
		
	    TokenStream stemFilter = new PorterStemFilter(new LowerCaseTokenizer(new StringReader(input)));
		CharTermAttribute token = stemFilter.getAttribute(CharTermAttribute.class);
		while (stemFilter.incrementToken()) {
			String word = token.toString();
			if (word.length() > 2 && word.length() < 30) {
				bagOfWords.add(word);
			}
			else
			{
				if (word.length() > 30) {
					System.out.println();
				}
			}
		}
		stemFilter.close();
	}
		
	static void removeNoisyWords() {
		try {
			BufferedReader bagOfWordsReader = new BufferedReader(new FileReader(DATASET_DIR + BAG_OF_WORDS_FILENAME));
			PrintWriter bowWriter = new PrintWriter(BAG_OF_WORDS_DENOISED);
			String bagofwordsline;
			int wordsFiltered = 0;
			while((bagofwordsline = bagOfWordsReader.readLine()) != null)
			{
				StringBuffer buffer = new StringBuffer(20);
				String[] words = bagofwordsline.split(" ");
				for(int i = 0; i<words.length; i++)
				{
					if(!noisyWordSet.contains(words[i]))
					{
						buffer.append(words[i]);
						buffer.append(" ");
					}
					else{
						wordsFiltered++;
					}
				}
				bowWriter.println(buffer.toString());
			}
			System.out.println("Noisy words filtered: " + wordsFiltered);
			bagOfWordsReader.close();
			bowWriter.close();
			
		} catch (IOException e) {
			// TODO Auto-generated catch block
			throw new RuntimeException(e);
		}
		
	}
	
	/**
	 * Removes the first and last words in every context
	 * @throws IOException 
	 */
	static void contextRadiusCleaner() throws IOException
	{
		BufferedReader inputReader = new BufferedReader(new FileReader(CONTEXT));
		PrintWriter outputwriter = new PrintWriter(DATASET_DIR + "ContextCleaned");
		String s;
		int lineNum = 0;
		Version matchVersion = Version.LUCENE_35; // Substitute desired Lucene version
		Analyzer analyzer = new StandardAnalyzer(matchVersion); // or any other analyzer
		while((s = inputReader.readLine())!=null)
		{
			int firstDelimiterIndex = s.indexOf(' '); int lastDelimiterIndex  = s.lastIndexOf(' ');
			if(firstDelimiterIndex > -1 && lastDelimiterIndex > -1)
			{
				s = s.substring(firstDelimiterIndex, lastDelimiterIndex);
				TokenStream tokenStream = analyzer.tokenStream("test", new StringReader(s));
				// retrieve the remaining tokens
				CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);

				while (tokenStream.incrementToken()) {
					outputwriter.print(token);
					outputwriter.print(" ");
				}
				outputwriter.println("");
				tokenStream.end();
				tokenStream.close();
			}
		}
		analyzer.close();
		inputReader.close();
		outputwriter.close();
	}
	
	static void stemContext() throws IOException
	{
		BufferedReader inputReader = new BufferedReader(new FileReader(DATASET_DIR + "ContextCleaned"));
		PrintWriter outputwriter = new PrintWriter(DATASET_DIR + "ContextCleanedKStemmed");
		String s;
		Version matchVersion = Version.LUCENE_35; // Substitute desired Lucene version
		while((s = inputReader.readLine())!=null)
		{
			TokenStream stemFilter = new KStemFilter(new LowerCaseTokenizer(matchVersion, new StringReader(s)));
			CharTermAttribute token = stemFilter.getAttribute(CharTermAttribute.class);
			while (stemFilter.incrementToken()) {
				String word = token.toString();
				if (word.length() > 2 && word.length() < 30) {
					outputwriter.print(token);
					outputwriter.print(" ");
				}
			}
			outputwriter.println("");
			stemFilter.close();
		}
		inputReader.close();
		outputwriter.close();
	}
	
	static void readNoisyWords()
	{
		BufferedReader noisyWordsReader;
		try {
			noisyWordsReader = new BufferedReader(new FileReader(NOISY_WORDS));
			noisyWordSet = new HashSet<>(TTable.findNum(NOISY_WORDS));
			String word;
			while((word = noisyWordsReader.readLine()) != null)
			{
				noisyWordSet.add(word.trim());
				
			}
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}
	
	private static BufferedReader idReader;
	private static BufferedReader contextReader;
	private static BufferedReader properNounReader;
	private static PrintWriter wordsWriter;
	private Set<String> bagOfWords = new HashSet<String>();
	static HashSet<String> noisyWordSet;
	//private BufferedReader titleReader;
	
	// Keeps a count of citing papers whose fertility is out of bound
	private int ignored = 0;
	static final int MAX_FERTILITY = 20;
	static final double LOWERBOUND = 1/MAX_FERTILITY;
	
	

}
