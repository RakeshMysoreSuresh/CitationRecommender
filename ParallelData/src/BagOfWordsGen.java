/**
 *
 */
import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintWriter;
import java.io.StringReader;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.en.EnglishMinimalStemFilter;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.util.Version;

/**
 * @author Rakesh_BCKUP
 * Generates a bag of words from all the citations in a citing paper. Each line corresponds to one citing paper.
 * Inputs: These files can be generated by directly exporting a column in database table to a text file. TOAD is an
 * useful tool for such operations.
 * 1. ID.txt: Each entry corresponds to the paperID of the citing paper
 * 2. Title.txt: The title of the cited paper
 * 3. Context.txt: The citation context of that citation
 * 4. ProperNoun file(OPTIONAL): If the proper nouns are already separated from other words in the citation context,
 * 	                             it can be excluded from the aggressive preprocessing step and chosen to be added to
 * 	                             the bag of words later.
 * 5. Fertility(OPTIONAL): The constant MAX_FERTILITY can be modified as required
 *
 * Outputs: Each line in this
 * 1. Bag of words file
 */
public class BagOfWordsGen {

	//"<ID file> <Raw Contexts> <Output BagofWords filename> "
	public BagOfWordsGen(String[] args) {

/*		if(args.length != 5){
			usage();
		}*/
		try {
			idReader = new BufferedReader(new FileReader(args[0]));
			//titleReader = new BufferedReader(new FileReader(args[1]));
			contextReader = new BufferedReader(new FileReader(args[1]));
			wordsWriter = new PrintWriter(new FileWriter(args[2]));
			System.out.println("Creating bag of words at:" + args[2]);
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}

	public BagOfWordsGen(String[] args, String properNounFile){
		this(args);
		try {
			properNounReader = new BufferedReader(new FileReader(properNounFile));
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}
	
	/**
	 * @param args
	 * @throws IOException
	 * @throws ClassNotFoundException
	 */

	public static void main(String[] args) throws IOException {

		BagOfWordsGen parallel = new BagOfWordsGen(new String[]{"ID", "Context", "BagOfWords"});
		parallel.generate();

	}
	/**
	 * Gather all words from all citation contexts in a given citing paper and write into a line
	 * Gather all title IDs(numbers) of all cited paperds from a given citing paper and write into a line
	 * @throws IOException
	 */
	public void generate() {
		int numCitation = 0;
		String curPaperId, context, noun; //title;
		String previousId = "";

		try {
			//Dummy reads to exclude the column name
			curPaperId = idReader.readLine(); // read the column name
			context = contextReader.readLine(); // read the column name
			//title = titleReader.readLine(); // read the column name
			if(properNounReader!=null){
				noun = properNounReader.readLine();//Dummy read
			}
			System.out.println(curPaperId);
			long startTime = 0, endTime = 0;
			startTime = System.currentTimeMillis();
			int percentage = 0;
			while ((curPaperId = idReader.readLine()) != null) {
				numCitation++;

				if (0 == numCitation % 12000) {
					endTime = System.currentTimeMillis();
					System.out.println("Time taken for " + (++percentage) + "% = "
							+ ((endTime - startTime) / 1000));
					// break;
				}
				if (curPaperId.equals(previousId)) {

					// Read one citation context and get bag of words
					context = contextReader.readLine();
					removeStopWordsStem(context);

				} else {
					if (bagOfWords.size() != 0) {
						// write bagsOfWords to a file
						// write citedPaperIds to a file
						writeToFile(bagOfWords, wordsWriter);
					}

					// Read one citation context and get bag of words
					context = contextReader.readLine();
					// Initialize Sets to read the new Paper citations
					bagOfWords.clear();
					int firstDelimiterIndex = context.indexOf(' '); int lastDelimiterIndex  = context.lastIndexOf(' ');
					if (firstDelimiterIndex > -1 && lastDelimiterIndex > -1) {
						context = context.substring(firstDelimiterIndex,
								lastDelimiterIndex);
						removeStopWordsStem(context);
						if (properNounReader != null) {
							noun = properNounReader.readLine();
							bagOfWords.addAll(Arrays.asList(noun.split(" ")));
						}
					}
					else
					{
						System.out.println("Ignoring: "+context);
					}
				}
				previousId = curPaperId;
			}

			// Write the last set of values
			if (bagOfWords != null) {
				// write bagsOfWords to a file
				// write citedPaperIds to a file
				writeToFile(bagOfWords, wordsWriter);
			}

			wordsWriter.close();
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}

	private static void writeToFile(Set<String> bagOfWords, PrintWriter writer) {

		Iterator<String> it = bagOfWords.iterator();
		String write = "";
		while (it.hasNext()) {
			write = write + " " + it.next();
		}
		writer.println(write);
	}

	private void removeStopWordsStem(String input) throws IOException {
		// input string
		Version matchVersion = Version.LUCENE_35; // Substitute desired Lucene version
		Analyzer analyzer = new StandardAnalyzer(matchVersion); // or any other analyzer
		TokenStream tokenStream = analyzer.tokenStream("test",
				new StringReader(input));
		// remove stop words
		tokenStream = new EnglishMinimalStemFilter(tokenStream);
		// retrieve the remaining tokens
		CharTermAttribute token = tokenStream
				.getAttribute(CharTermAttribute.class);

		while (tokenStream.incrementToken()) {
			bagOfWords.add(token.toString());
		}

		tokenStream.end();
		tokenStream.close();
		analyzer.close();
	}
	
	private static BufferedReader idReader;
	private static BufferedReader contextReader;
	private static BufferedReader properNounReader;
	private static PrintWriter wordsWriter;
	private Set<String> bagOfWords = new HashSet<String>();
	//private BufferedReader titleReader;
	
	// Keeps a count of citing papers whose fertility is out of bound
/*	private int ignored = 0;
	static final int MAX_FERTILITY = 20;
	static final double LOWERBOUND = 1/MAX_FERTILITY;
	
	private void writeToFile() {
		double fertility = bagOfWords.size()/citedPaperIds.size();

		if (!((fertility>MAX_FERTILITY)||(fertility<LOWERBOUND))) {
			Iterator<String> it = bagOfWords.iterator();
			String write = "";
			while (it.hasNext()) {
				write = write + " " + it.next();
			}
			wordsWriter.println(write);
			Iterator<Integer> it2 = citedPaperIds.iterator();
			write = "";
			while (it2.hasNext()) {
				write = write + " " + it2.next();
			}
			titleWriter.println(write);
		}
		else{
			ignored++;
		}
	}*/

}
